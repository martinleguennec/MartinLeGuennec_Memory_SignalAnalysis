---
title: "R Project"
author: "Martin Le Guennec"
date: "2023-05-11"
output:
  html_document:
    self_contained: yes
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: yes
    theme: united
    highlight: tango
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
    highlight: tango
link-citations: yes
csl: apa.csl
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Code description

This code compares several model of prediction on a data set to find the one who is the better predictor.

The data set that will be used in this code is contained in the directory `DAT` and is names `data_R.csv`

Please make sure that there is a file named `apa.csl` and another file named `references.bib`, they are used to cite the articles mentionned in this code.

The file you are currently reading is `main.rmd`, it is the only code you have to run in order to get the full analysis. You can just run all the code or run the blocks of code one by one, some of the parameters can be changed to make the processing time faster. You can also simply `knit` this script to pdf or html.

## Organization of the code

First we load the libraries used in this code and we create a function named `TestModel` that will be used many times in this code.

Then, you will find a part that explains the context and the objective of this code.

Finally, the major part of the code consists of creating models and comparing them

```{r install_libraries, message = FALSE, warning = FALSE, include=FALSE}
# If some of these libraries are already installed and you don't want to install
# them back, please comment concerned lines in this block
# install.packages("rstatix")
# install.packages("report")
# install.packages("tidyverse")
# install.packages("cowplot")
# install.packages("lmerTest")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("randomForest")
```

```{r library_loading, message=FALSE, warning=FALSE, include = FALSE}
# Load the useful libraries

# For statistical tests 
library(rstatix)
library(report)        # Write the results
# Organizing the data and make graphs
library(tidyverse)     # For ggplot and tidyr
# library(ggpubr)        # Add statistical values to graph
# library(ggside)        # Add distribution on the side
library(cowplot)       # Plot graphs side by side
# For the models
library(lmerTest)      # Linear mixed models
library(rpart)         # Make decision trees
library(rpart.plot)    # Plot them
library(randomForest)  # Make random forests
```

```{r functions, include = FALSE}
TestModel <- function(model, data, formula, number.itteration, 
                      cp.tree = 0.01, minsplitTree = 20, ntree.forest = 200){
  # Calculates the RMSE of a model on a data set 
  #
  # Inputs:
  #    model              : character   Indicates the model used, either "mean"
  #                                     , "mod.lin", "tree" or "forest"
  #    data               : data frame  The data set on which the model is
  #                                     trained, then predicted
  #    formula            : formula     The formula used to train the model, of
  #                                     the form "y ~ x + z + ..." where x, y
  #                                     and z are columns of data
  #    number.itteration  : numeric     The number of itteration to calculate
  #                                     the RMSE of the model
  #    ###### OPTIONNAL #######
  #    cp.tree            : numeric     Only if the model is "tree", sets the
  #                                     complexity (number of branches) of the
  #                                     model
  #    minsplitTree       : numeric     Only if the model is "tree", sets the 
  #                                     number of examples to observe to divide
  #                                     a leaf
  #    ntree.forest       : numeric     Only if the model is "forest", sets the
  #                                     number of tree in the forest
  #
  # Outputs :
  #    rmse.list          : list        Dimension 1 x number.itteration,
  #                                     contains the RMSE values of each
  #                                     itteration of the model
  #
  # Description :
  #    TestModel : separates the data in training and testing data sets. The
  #    model is fitted on the training data set based on the formula, then
  #   predicted on the testing data set (the MV is predicted here). We then
  #   calculate the RMSE to see the accuracy of the model. The process is
  #   repetead as many time as the number.itteration
  
  # Author
  #    Martin Le Guennec - Univ. Montpellier - France
  # Versions
  #    Version 1.0.0 -- M. Le Guennec -- 2023-05-11
  #      First version
  #    Version 1.1.0 -- M. Le Guennec -- 2023-05-19
  #      Add the minsplitTree input
  #      Setting default values for the optionnal inputs
  
  
  rmse.list <- c()  # Empty list, it will be filled at each itteration
  
  for (itteration in 1 : number.itteration){
    
    # Shuffle the data so that they are in random order 
    # and the model is fitted on different data each itteration
    data <- data[sample(nrow(data)), ]
    
    # Prepare the train and test data sets
    # We take 75% of the data to train the model
    kNumberTrainLines <-  0.75 * (nrow(data))  
    train <- data[1 : kNumberTrainLines, ]
    test <- data[kNumberTrainLines : nrow(data), ]
    
    # We fit the model on the training data based on the model type 
    # specified in the inputs
    fitted_model <- switch(
      model, 
      mean    = mean(train$MV),
      mod.lin = lm(formula, data = train),
      tree    = rpart(formula, data = train, cp = cp.tree, 
                      minsplit = minsplitTree),
      forest  = randomForest(formula, data = train, ntree = ntree.forest),
      lm.mixt = lmer(formula = formula, data = train)
    )
    
    if (model == "mean"){
      # If the model is "mean", we don't want to predict
      # we just take the mean of train values
      prediction <- fitted_model  
    } else {
      prediction <- predict(fitted_model, test)
    }
      
    rmse <- sqrt(mean((prediction - test$MV)^2))
    rmse.list <- c(rmse.list, rmse)
  }
  
  return(rmse.list)
}
```

```{r, include = FALSE}
SortPvalues <- function(model, data, pvalue){
  
  pvalue.to.suppress <- TRUE

  while (pvalue.to.suppress){
    
    # We set this on true by default, this way, if no variable isn't significant, the loop stops
    pvalue.to.suppress <-  FALSE
    
    # Empty list that we will fill with variables names afterwards
    significant.variables <- c()
  
    # Loop trough the variable and verify if their pvalue is < 0.05
    # If it is the case, we add the name of the variable list create before
    for (nVariable in 2:length(model$coefficients)){
      
      pvalue.variable <- summary(model)$coefficients[[nVariable,4]]
      name.variable <- names(model$coefficients)[[nVariable]]
      
      
      if (!grepl("^(ID|set)", name.variable)){  
        if (pvalue.variable < pvalue){
        # We don't want to take ID and set factors because we keep them anyways
        
          significant.variables <- c(significant.variables, name.variable)
        } else {
          # If there is one p-value > 0.05, we must do one more loop
          pvalue.to.suppress <- TRUE
        }
      } 
    }
    
    # Create a new estimate with the variables that were significantly linked to the MV 
    estimate.updated <-  as.formula(paste( 
      "MV ~ ", 
      paste(significant.variables, collapse = " + "),  
      " + ID + set"
      ))
    
    # Fit a new model based on the new estimate
    model <- lm(estimate.updated, data = data)
  }
  
  return(model)
}

```

```{r, include = FALSE}
##################### I. Create paths useful for the code #####################
WRK_PATH <- here::here()  # Create the work path
setwd(WRK_PATH)           # Set the working directory


############################## II. Load the data ##############################
data <- read.csv("DAT/data_R.csv")

# Keep only the useful values in the data frame
# The reasons are discussed later
data <- data[, c(1:5, 10:14, 19:23, 28:32, 37:41, 46:50)]

# We rename some of the variables so that it is easier to code
colnames(data)[c(2, 3, 5)] <- c("ID", "set", "MV")

# Identify the factors
data$session <- factor(data$session)
data$ID <- factor(data$ID)

# Unify the names in the dataframe
data$set[data$set == "v1mspre"] <- "PRE"
data$set[data$set == "v1mspost"] <- "POST"

########################## Prepare several dataframe ##########################
# We don't take pre and post values (not useful in this code)
sessions <- data %>%
  filter(set != "PRE" & set != "POST") %>%
  ungroup()

sessions$set <- as.numeric(sessions$set)
# Split the data frame by sessions
# We chose to call the session at 0.5 m/s "session 1"
# and the session at 0.8 m/s "session 2" by convention
session.1 <- sessions %>%
  filter(session == "1")
#There is one outlier
session.1 <- session.1 %>%
  filter(ID != "01PrAy" | set != 5 | repetition != 7)


session.2 <- sessions %>%
  filter(session == "2")
```

\newpage

# Context

Despite our growing knowledge of the mechanisms of fatigue during physical effort, traditional strength training does not use any objective means to characterize these. However, individualization and the utilization of scientific advancements have become pivotal in physical preparation. Recognizing this, @TeikariPietrusz2021 propose the incorporation of objective measurements to quantify fatigue during training. By leveraging these quantifiable indicators, coaches and athletes can gain valuable insights into the physiological responses to resistance training, enabling more informed and targeted training strategies. This integration of objective signals holds the potential to enhance performance outcomes and optimize training adaptations by tailoring workouts to the individual needs of athletes.

## The experiment

The objective of my experiment was to determine if it was possible to predict fatigue from physiological measurements during different resistance training modalities.

To do this, 7 participants performed two different measurement sessions where they performed squat movements at maximal intented speed. The two sessions differed in the initial mean concentric velocity (MV) that the participants were asked to achieve. This MV was measured using a linear position transducer.

In the first session, they performed squats with the load that allowed them to lift the bar at approximately 0.8 m.s<sup>-1</sup>; and in the second session, it was with the load that allowed them to lift the bar at approximately 0.5 m.s<sup>-1</sup>. This speed had to be reached during the first 3 repetitions of a set, otherwise the weight was increased or decreased accordingly.

They continued to perform reps until they lost 20% velocity from the MV of the fastest rep of the set. When they reached this 20% threshold, they had completed a set.

They had to perform as many sets as possible to reach their training load. This training load was calculated for each repetition performed as follow : $$WL = \sum^n \frac{m^2 \times g \times ROM}{MV \times h}$$

where $WL$ is the workload ; $n$ is the number of repetitions performed ; $m$ is the weight lifted (in % repetition maximum) and is normalized with respect to the theoretical maximal weight that the subject can lift for one repetition on the squat ; $g$ is the acceleration of the gravity at the Earth's surface and is worth 9.81 m.s<sup>-2</sup> ; $ROM$ is the range of motion (in m) of the barbell during the n<sub>th</sub> repetition ; $MV$ is the mean concentric velocity (in m.s<sup>-1</sup>) ; and $h$ height is the height of the subject (in m) and is used to normalized the ROM.

While they perform the squats, their muscular electrical activity is measured with electromyography (EMG), their cortical electrical activity is measured with electroencephalography (EEG), and their muscular metabolic activity is measured with near-infrared spectroscopy (NIRS).

### Why measuring velocity ?

It has been shown that the use of velocity allows an objective and precise prescription of training volume and work intensity :

-   It allows a quick and precise estimation of the intensity, allowing an individualization of the training and an adaptation to the athlete's daily shape [@Weakley&al.2021 ; @FlanaganJovanovic2014]

-   Bar speed is also a good indicator of fatigue because of its relationship with other mechanical and metabolic variables [@Sanchez-MedinaGonzalez-Badillo2011]

For these reasons, it is the variable that we will use to estimate the level of fatigue of the subjects during the session.

### EMG measurements

The EMG were measured on 9 different muscles :

-   Gastrocnemius medialis (GaMe)

-   Gastrocnemius lateralis (GaLa)

-   Biceps femoris (BiFe)

-   Semi-tendinous (SeTe)

-   Quadriceps' vastus lateralis (VaLa)

-   Quadriceps' rectus femoris (ReFe)

-   Quadriceps' vastus medialis (VaMe)

-   Gluteus maximus (GlMa)

-   Lumber extensors (ExLo)

In this analysis, we will only focus on VaLa, ReFe, VaMe, GlMa and ExLo because the signal for the other were very noisy, therefore, the extracted variables must be erroneus.

For each of these variables, we measured the mean instantaneous mean frequency (IMNF) for each repetition, the mean value of the RMS enveloppe (meanRMS), the area under the curve of the RMS (aucRMS) and the maximum value of the RMS (maxRMS). These values were calculated in Matlab.

\newpage

## Aim of the code

The aim of this code is to predict the fatigue -- estimated from the MV -- thanks to the physiological measurements' values. Normally, we should use EEG, EMG and NIRS measurements but I didn't have the time to treat NIRS and EEG data. Therefore, we will lean only on EMG data to predict the fatigue.

Thus, with this code, we will compare different models with different parameters to find the better to predict MV using $$\hat{Y} = \hat{f}(X)$$ where $\hat{Y}$ is the prediction of $Y$ -- in our context, the MV -, $\hat{f}$ is our estimate model of prediction, and $X$ is the set of inputs measured -- here the EMG data - used to predict $Y$ [@James&al.2021].

For this code, we will compare two estimate model of prediction with different methods

-   The first one will predict the MV only with EMG values will be named `estimate.1` $$\hat{MV} = \sum^{muscle} \left( \hat{f}(IMNF_{muscle}) + \hat{f}(meanRMS_{muscle}) + \hat{f}(aucRMS_{muscle}) + \hat{f}(maxRMS_{muscle} + \hat{f}(t2maxRMS_{muscle}) \right)$$

-   The second one will predict the MV with the EMG values, and with the subject ID, set number and repetition number as co-variables. It will be named `estimate.2` $$\begin{split} \hat{MV} = &\sum^{muscle} \left( \hat{f}(IMNF_{muscle}) + \hat{f}(meanRMS_{muscle}) + \hat{f}(aucRMS_{muscle}) + \hat{f}(maxRMS_{muscle}) + \hat{f}(t2maxRMS_{muscle}) \right) \\ &+ subjectID + setNumber \end{split}$$

```{r estimates}
estimate.1 <- as.formula(
  MV ~ 
    IMNF_VaMe + IMNF_ReFe + IMNF_VaLa + IMNF_GlMa + IMNF_ExLo + 
    meanRMS_VaMe + meanRMS_ReFe + meanRMS_VaLa + meanRMS_GlMa +
    meanRMS_ExLo + 
    aucRMS_VaMe + aucRMS_ReFe + aucRMS_VaLa + aucRMS_GlMa + aucRMS_ExLo +
    maxRMS_VaMe + maxRMS_ReFe + maxRMS_VaLa + maxRMS_GlMa + maxRMS_ExLo +
    t2maxRMS_VaMe + t2maxRMS_ReFe + t2maxRMS_VaLa + t2maxRMS_GlMa + t2maxRMS_ExLo
  )

estimate.2 <- as.formula(
  MV ~ 
    IMNF_VaMe + IMNF_ReFe + IMNF_VaLa + IMNF_GlMa + IMNF_ExLo + 
    meanRMS_VaMe + meanRMS_ReFe + meanRMS_VaLa + meanRMS_GlMa +
    meanRMS_ExLo + 
    aucRMS_VaMe + aucRMS_ReFe + aucRMS_VaLa + aucRMS_GlMa + aucRMS_ExLo +
    maxRMS_VaMe + maxRMS_ReFe + maxRMS_VaLa + maxRMS_GlMa + maxRMS_ExLo +
    t2maxRMS_VaMe + t2maxRMS_ReFe + t2maxRMS_VaLa + t2maxRMS_GlMa + t2maxRMS_ExLo +
    ID + set
  )
```

\newpage

# Compare models

As mentioned above, we will compare the two estimates but we will also compare different methods :

-   [Linear model]

-   [Linear mixed models]

-   [Regression trees]

-   [Random forest]

For each of these models, we will try different parameters with

-   The two estimates

-   Different data sets : the full data set or split into sessions

    -   Given that the two session don't correspond to the same modality of resistance training, maybe that fitting a model by session will allow for a better prediction than if fit it for the whole data set

**Set the number of iteration**

We set a number of 100 iteration that will be common to all models. This number corresponds to the number of times that the data set will be shuffled and then the model trained on this basis.

The higher this number, the higher the prediction accuracy but there is a risk of over fitting the data (*i.e.* the model performs exceptionally well on the training data but fails to generalize well to unseen or new data) and increases the calculation time. Inversely, a low number of iteration per model will certainly reduce the accuracy of its prediction.

```{r}
kNumberRun <- 100
```

The performance of a model is assessed by the root mean square error, calculated as ; $$RMSE = \sqrt{\frac{1}{n} \sum^n (y_i - \hat{y}_i)^2}$$

Smaller RMSE values indicate better predictive accuracy.

## Disclaimer

The interpretation of the results is based of the data that I have obtained when I made this document. However, given that each iteration is made on a random sample of the data set, it is possible that the results differ from what I have seen.

I would like to emphasize that the results I discuss in this script were obtained by making a high number of run (100). These results seem therefore pertinent and I invite you to verify but the processing time can be long.

\newpage

# Linear model

First, we evaluate the performances of a linear model. The linear model, of multiple linear regression model, assumes that there is approximately a linear relationship between each $X_p$ and $Y$. The aim of this model, is to determine the coefficients

## Choose parameters

We will compare a model with the estimate model 1 (`estimate1`), and a model with the estimate model 2 (`estimate2`).

In addition, we are going to compare the performances of each linear model on the entire data set (`entire`,) then on each session individually (`session1` and `session2`).

```{r, warning = FALSE}
# Create data frame with the different linear models
comparison.modlin <- data.frame(
  dummy = TestModel(
    model = "mean", 
    data = sessions, 
    number.itteration = kNumberRun
    ),
  estimate1.entire = TestModel(
    model = "mod.lin", 
    data = sessions, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate2.entire = TestModel(
    model = "mod.lin", 
    data = sessions, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    ),
  estimate1.session1 = TestModel(
    model = "mod.lin",
    data = session.1,
    formula = estimate.1,
    number.itteration = kNumberRun
  ),
  estimate1.session2 = TestModel(
    model = "mod.lin",
    data = session.2,
    formula = estimate.1,
    number.itteration = kNumberRun
  ),
  estimate2.session1 = TestModel(
    model = "mod.lin",
    data = session.1,
    formula = estimate.2,
    number.itteration = kNumberRun
  ),
  estimate2.session2 = TestModel(
    model = "mod.lin",
    data = session.2,
    formula = estimate.2,
    number.itteration = kNumberRun
  )
) %>% 
  pivot_longer(cols = everything(), 
               names_to = "model", values_to = "accuracy") %>%
  separate(model, into = c("estimate", "dataset"), sep="\\.", remove = F)

# Compare the models
comparison.modlin %>%
  ggplot(aes(x = model, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  ggtitle(
    "Comparison of linear models", 
    subtitle = "Different estimates and data sets"
    ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )

```

All models allow a reliable prediction because they are more precise than the dummy prediction.

\newpage

### Estimate comparison

We are going to compare the models by estimate first.

```{r}
comparison.modlin %>%
  filter(model != "dummy") %>%
  ggplot(aes(x = estimate, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  facet_grid(cols = vars(dataset)) +
  ggtitle(
    "Comparison of linear models", 
    subtitle = "Different estimates"
    ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )
```

Using the `estimate.2` improve the accuracy of the model.

\newpage

### Data set comparison

We are going to compare the accuracy of the model with the entire data set vs. with separate data set for each session.

```{r}
comparison.modlin %>%
  filter(estimate == 'estimate2') %>%
  ggplot(aes(x = dataset, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  ggtitle(
    "Comparison of linear models", 
    subtitle = "Different data sets"
    ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )
```

Separating the data set by sessions seem to improve the accuracy of the model.

\newpage

## Interpretation

### Session 0,8 m/s

Now that we have identified the best parameters for our regression model, we can fit it on the whole data set and analyse the results. We will proceed by session.

```{r}
# Fit a linear model on the whole data set with the parameters decided
lm.session1 <- lm(estimate.2, data = session.1)
# Display the summary
summary(lm.session1)
```

In our model, at least one of the predictor variables is significantly related to the MV, *F*(32, 454) = 48.32, *p* \< 0.0001.

This model explains 77.3 % of the variance (75.7 % after applying a correction).

**The section that follows was not made for the memory. It is however an area to improve the prediction of the model**

As we can see however, many coefficients aren't significant. Therefore, we can remove them from the model.

```{r}
# Clear the model
lm.session1.updated <- SortPvalues(lm.session1, session.1, 0.05)
# Extract his new formula
estimate.updated<- formula(lm.session1)
# Display the model
summary(lm.session1.updated)
```

This way our model is simpler and we don't lose much prediction power despite having less predictors. We have a adjusted R2 at 0.754 (after adjustment) instead of 0.757 with all the values. Therefore, we gain in simplicity and not even 1% of the variance explanation.

\newpage

### Session 0,5 m/s

Now, let's do the same for the session at 0,5 m/s.

```{r}
# Fit a model over the whole data set
lm.session2 <- lm(estimate.2, data = session.2)
# Display the model
summary(lm.session2)
```

In our model, at least one of the predictor variables is significantly related to the MV, *F*(32, 99) = 19,6, *p* \< 0.0001.

This model explains 86.4 % of the variance (82.0 % after applying a correction).

```{r}
# Clear the model
lm.session2.updated <- SortPvalues(lm.session2, session.2, 0.05)
# Extract his new formula 
estimate.updated<- formula(lm.session2.updated)
# Display the model
summary(lm.session2.updated)
```

This time we lost a lot of variance explanation by suppressing the values because we went from a R2 at 0.820 (after correction) to a R2 at 0.654. Therefore, we will do the sorting in two steps : before we suppress all the pvalues inferior to 0.10, them we will make a final sorting with the pvalues \< 0.05.

```{r}
# First sort the values with p < 0.1
lm.session2.updated.2steps <- SortPvalues(lm.session2, session.2, 0.1)
# Then sort the values with p < 0.05
lm.session2.updated.2steps <- SortPvalues(lm.session2.updated.2steps, 
                                          session.2, 0.05)
# Display the model
summary(lm.session2.updated.2steps)
```

Thanks to this one more step, we managed to get back to a R2 of 0.811 and a simpler model.

\newpage

## Conclusion

Using the `estimate.2` and splitting the data set by session improves the accuracy of the model's prediction.

We have the choice, once we have fitted the model on the whole data set, to keep the model as it is or to clean it. To do so, we can keep only the variables that explain a significant part of the variance. To do that, we can keep directly the variables whose p value is \< 0.05, or do that in several steps.

I think that one way of improving the model is to introduce some interactions between variables. A analysis of the synergies would be necessary before to determine which interactions we should include (for example like @smale2016 ).

Introducing more variables would also be great to have a chance of having one truly linked with the MV. Having the NIRS data would greatly improve their prediction first, then EEG data two. We can also extract more indicators from the EMG data.

\newpage

# Linear mixed models

First, we need to build a linear mixed model by taking into account set and ID as random effects. We make the assumption that intercept and slope can be modified by the subject who performs the squat, and the number of sets performed before.

Therefore, we will use the following formula : $$MV \sim EMG_{variables} + (1 \rvert ID/set)$$

```{r}
estimate.lmer <- as.formula(
  MV ~ 
    IMNF_VaMe + IMNF_ReFe + IMNF_VaLa + IMNF_GlMa + IMNF_ExLo + 
    meanRMS_VaMe + meanRMS_ReFe + meanRMS_VaLa + meanRMS_GlMa +
    meanRMS_ExLo + 
    aucRMS_VaMe + aucRMS_ReFe + aucRMS_VaLa + aucRMS_GlMa + aucRMS_ExLo +
    maxRMS_VaMe + maxRMS_ReFe + maxRMS_VaLa + maxRMS_GlMa + maxRMS_ExLo +
    t2maxRMS_VaMe + t2maxRMS_ReFe + t2maxRMS_VaLa + t2maxRMS_GlMa +
    t2maxRMS_ExLo  +
    (1 | ID / set)
  )
```

## Session 0,8 m/s

```{r}
mod.lmer.session1 <- lmer(formula = estimate.lmer, data = session.1) 
summary(mod.lmer.session1)
```

The model's total explanatory power is substantial (condition R2 = 0.85) and the part related to the fixed effects alone (marginal R2) is of 0.44.

```{r}
mod.lmer.session2 <- lmer(formula = estimate.lmer, data = session.2) 
summary(mod.lmer.session2)
```

The model's total explanatory power is substantial (condition R2 = 0.96) and the part related to the fixed effects alone (marginal R2) is of 0.43.

# Compare the two linear model

Now that we have fitted the linear models (multiple regression and linear mixed models), we are going to visualize the coefficients. This way, it will be easier to interpret.

```{r}
graph.coeff <- data.frame(as.data.frame(report(lm.session1)))[2:26,c(1,2,4,5,8)]
graph.coeff$model <- "RLM"
graph.coeff$session <- "Session 0,8 m/s"

temp <- data.frame(as.data.frame(report(lm.session2)))[2:26,c(1,2,4,5,8)]
temp$model <- "RLM"
temp$session <- "Session 0,5 m/s"
graph.coeff <- rbind(graph.coeff, temp)

temp <- data.frame(as.data.frame(report(mod.lmer.session1)))[2:26,c(1,2,4,5,8)]
temp$model <- "MLM"
temp$session <- "Session 0,8 m/s"
graph.coeff <- rbind(graph.coeff, temp)

temp <- data.frame(as.data.frame(report(mod.lmer.session2)))[2:26,c(1,2,4,5,8)]
temp$model <- "MLM"
temp$session <- "Session 0,5 m/s"
graph.coeff <- rbind(graph.coeff, temp)

graph.coeff <- graph.coeff %>%
  separate(Parameter, into= c("Variable", "Muscle"), sep = " ")

graph.coeff$signif <- ifelse(graph.coeff$p < 0.001, "< 0,001",
                             ifelse(graph.coeff$p < 0.01, "< 0,01",
                                    ifelse(graph.coeff$p < 0.05, "< 0,05", 
                                           "> 0,05")))

graph.coeff$model <- factor(graph.coeff$model)
graph.coeff$session <-  factor(graph.coeff$session)
graph.coeff$Variable <- factor(graph.coeff$Variable, 
                               levels = c("IMNF", "aucRMS", "meanRMS", 
                                          "maxRMS", "t2maxRMS"))

p <- graph.coeff %>% 
  ggplot(aes(x = Muscle, y = Coefficient, 
             shape = signif, size = signif, color = model, linewidth = signif)
         ) +
  scale_size_manual(values = c(1.2, 1.2, 1.2, .5)) +
  scale_linewidth_manual(values = c(1.2, 1.2, 1.2, .7)) +
  geom_hline(yintercept = 0, linetype = "dotted", linewidth = .7) +
  geom_pointrange( aes(ymin = CI_low, ymax = CI_high),
                   position = position_dodge2(reverse = TRUE, width = .5)
                   ) +
  scale_shape_manual(values = c(21, 13, 16, 4)) +
  coord_flip() +
  xlab(" ") +
  theme(
    axis.text.y = element_text(size = 10),
    axis.title.x = element_text(size = 20, face = "bold"),
    axis.text.x = element_text(hjust = 1, size = 10),
    strip.text = element_text(size = 12),
    legend.text = element_text(size = 12),
    legend.title = element_text(size = 15),
    legend.key.size = unit(20, "pt")
  ) +
  facet_wrap(Variable ~ session, ncol = 2, scales = "free")  

ggsave("RES/linearModels_coefficients.png", p,
       width = 3000, height = 4500, units = "px")
```

This graph is pretty big, and can't be seen properly in this pdf. I refer you to the figure `linearModels_coefficients` in the `RES` repository to see it.

\newpage

# Regression trees

Regression tree partitions recursively the input data into subsets based on the values of the input feature. At each step, the algorithm selects a feature $X_p$ and a split point that optimally divides the data, aiming to minimize the variability of the target variable $Y$ (here MV) within each resulting subset.

## Choose the parameters

First, we will compare the accuracy of the tree's prediction with the estimate model 1 (`estimate1`) vs. with the estimate model 2 (`estimate2`). Then, we are going to compare the performances of each linear model on the entire data set (`entire`,) then on each session individually (`session1` and `session2`).

```{r}
# Tree comparison with default complexity and number of example to divide 
# a leaf
comparison.trees <- data.frame(
  dummy = TestModel(
    model = "mean", 
    data = sessions, 
    number.itteration = kNumberRun
    ),
  estimate1.entire = TestModel(
    model = "tree", 
    data = sessions, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate1.session1 = TestModel(
    model = "tree", 
    data = session.1, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate1.session2 = TestModel(
    model = "tree", 
    data = session.2, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate2.entire = TestModel(
    model = "tree", 
    data = sessions, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    ),
  estimate2.session1 = TestModel(
    model = "tree", 
    data = session.1, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    ),
  estimate2.session2 = TestModel(
    model = "tree", 
    data = session.2, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    )
) %>%
  pivot_longer(cols = everything(), 
               names_to = "model", values_to = "accuracy") %>% 
  separate(model, into = c("estimate", "dataset"), sep="\\.", remove = F)


# Compare the models
comparison.trees %>% 
  ggplot(aes(x = model, y = accuracy)) + 
  geom_boxplot() +
  theme_classic() +
  ggtitle("Comparison of tree models", 
          subtitle = "Different estimates and data sets"
          ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )
```

All models allow a reliable prediction because they are more precise than the dummy prediction.

\newpage

### Estimate comparison

We are going to compare the models by estimate first.

```{r}
comparison.trees %>%
  filter(model != "dummy") %>%
  ggplot(aes(x = estimate, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  facet_grid(cols = vars(dataset), scales = "free_x") +
  ggtitle(
    "Comparison of tree models", 
    subtitle = "Different estimates"
    ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )
```

Using the `estimate.2` improve the accuracy of the model.

\newpage

### Data set comparison

We are going to compare the accuracy of the model with the entire data set vs. with separate data set for each session.

```{r}
comparison.trees %>%
  filter(estimate == "estimate1") %>%
  ggplot(aes(x = dataset, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  ggtitle("Comparison of tree models", 
          subtitle = "Different data sets"
          ) +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5))
```

Separating the data set by sessions seem to improve the accuracy of the model.

\newpage

### Complexity and number of examples

In addition, we will also try different complexity parameters and number of examples for the tree.

The complexity controls the number of splits, or branches, in the tree.

-   A low complexity means fewer branch ; it allows an easier interpretation and it is less prone to over fitting, but the predictive accuracy is reduced

-   A high complexity means more branch ; it may capture more intricate patterns, but is harder to interpret and more susceptible to over fitting

Let's see an example.

```{r tree_comparison, warning=FALSE}
# To increase the complexity, we reduce the cp
tree.low.complexity    <- rpart(estimate.1, sessions, cp = 0.1)
tree.medium.complexity <- rpart(estimate.1, sessions, cp = 0.01)
tree.high.complexity   <- rpart(estimate.1, sessions, cp = 0.001)

par(mfrow = c(1,3))

prp(tree.low.complexity, main = "Low complexity", cex.main = 2)
prp(tree.medium.complexity, main = "Medium complexity", cex.main = 2)
prp(tree.high.complexity, main = "High complexity", cex.main = 2)
```

The number of examples controls the minimum number of observations that must exist in the data set in order for a split to be attempted.

Let's determine the optimal complexity parameter to predict our data sets. We will compare :

-   Complexity parameters : 0, 0.0001, 0.001 and 0.01

-   Number of examples : lies in [10, 20, ... 100]

```{r}
# Make a data frame with the models
complexity.parameters <- c(0, 0.0001, 0.001, 0.01)
numexample.parameters <- seq(10, 100, by = 10)

# Create empty data frame, it will store the RMSE values
comparison.tree.hyperparameter <- data.frame()

# First loop for session 1
for (nComplexity in complexity.parameters){
  for(nExamples in numexample.parameters){
    
    # Create the model
    current.model <- TestModel(
      model = "tree",
      data = session.1,
      formula = estimate.2,
      number.itteration = kNumberRun,
      cp.tree = nComplexity,
      minsplitTree = nExamples)
    
    # Create a tree name to identify it in the data frame
    tree.name <- paste("Sess 1 cp", nComplexity, "div", nExamples)
    
    comparison.tree.hyperparameter <- rbind(
      comparison.tree.hyperparameter,
      data.frame(name = tree.name, session = "Session 1", # General info
                 cp = nComplexity, div = nExamples,       # What we test
                 t(unlist(current.model))                 # The RMSE values
                 )
      )
  }
}

# Second loop for session 2
for (nComplexity in complexity.parameters){
  for(nExamples in numexample.parameters){
    
    # Create the model
    current.model <- TestModel(
      model = "tree",
      data = session.2,
      formula = estimate.2,
      number.itteration = kNumberRun,
      cp.tree = nComplexity,
      minsplitTree = nExamples)
    
    # Create a tree name to identify it in the data frame
    tree.name <- paste("Sess 2 cp", nComplexity, "div", nExamples)
    
    comparison.tree.hyperparameter <- rbind(
      comparison.tree.hyperparameter, 
      data.frame(name = tree.name, session = "Session 2",  # General info
                 cp = nComplexity, div = nExamples,        # What we test
                 t(unlist(current.model))                  # The RMSE values
                 )
      )
  }
}
 
# Data frame in long format
comparison.tree.hyperparameter <-  comparison.tree.hyperparameter %>%
  pivot_longer(cols = 5:ncol(comparison.tree.hyperparameter), 
               names_to = "tryNumber", values_to = "rmse")

# One of the cp is in scientific notation, it make it difficult to read
comparison.tree.hyperparameter$cp <- factor(
  as.character(comparison.tree.hyperparameter$cp), 
  levels = c("0", "1e-04", "0.001", "0.01"), 
  labels = c("0", "0.0001", "0.001", "0.01"))

# Make plots to visualize
plot.trees.session1 <- comparison.tree.hyperparameter %>%
  filter(session == "Session 1") %>%
  ggplot(aes(x = cp, y = rmse)) +
  geom_boxplot() +
  facet_grid(cols = vars(div), scales = "free_x") +
  ggtitle("Comparison of trees for session 1 prediction",
          subtitle = "Complexity and number of examples needed") +
  xlab("Complexity parameter") + ylab("RMSE") +
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )

plot.trees.session2 <- comparison.tree.hyperparameter %>%
  filter(session == "Session 2") %>%
  ggplot(aes(x = cp, y = rmse)) +
  geom_boxplot() +
  facet_grid(cols = vars(div), scales = "free_x") +
  ggtitle("Comparison of trees for session 2 prediction",
          subtitle = "Complexity and number of examples needed") +
  xlab("Complexity parameter") + ylab("RMSE") +
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )

plot_grid(plot.trees.session1, plot.trees.session2)
```

Based on the graphs of the two sessions, we will chose `cp = 0.001` and `nb examples = 20`.

\newpage

### Conclusion

The chosen parameters are the following :

-   The `estimate.2`

-   Split the data sets by sessions

-   The complexity is set at 0.001

-   The number of examples needed to split a new branch is 20

\newpage

## Interpretation

Now, let's see what variables are the most useful to predict the MV with our data sets.

First for session 1

```{r}
# Make tree on the whole session 1 data set 
tree.session1 <- rpart(formula = estimate.2, data = session.1, 
                       cp = 0.001, minsplit = 20)

# Plot the relative error as a function of cp parameter
plotcp(tree.session1, upper = "splits")
```

We will now prune the tree with the smallest relative SSE xerror to balance predictive power with simplicity.

The minimum error seem to be with cp = 0.001533046. However, the maximum CP under the dashed lineis at 4 splits. We will use this to prune the tree and then interpret it.

```{r}
# Find the new cp parameter value
cp.tree.session1 = tree.session1$cptable[tree.session1$cptable[,2] == 12, "CP"]

# Plot a tree with the new cp
rpart.plot(prune(tree.session1, cp = cp.tree.session1),
           type = 5,     # Displays variable name in interior nodes
           extra = 101,  # Displays number and percentage of observation
           space = 0,    # Deletes the space and make the text bigger
           main = "Session 1"
           )
```

Now we cant see the variable importance for this tree

```{r}
imp.tree1 <- prune(tree.session1, cp = cp.tree.session1)$variable.importance %>%
  data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Importance = ".")

imp.tree1 <- imp.tree1[order(-imp.tree1$Importance),] 
imp.tree1$rank <- rank(-imp.tree1$Importance, ties.method = "min")

imp.tree1 %>%  ggplot(aes(x = fct_reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  theme_classic() +
  coord_flip() +
  xlab(" ")

```

Now let's see the accuracy of the model

```{r}
pred.tree.session1 <- predict(prune(tree.session1, cp = cp.tree.session1),
                              data = session.1)

data.frame(Predicted = pred.tree.session1, Actual = session.1$MV) %>%
  ggplot(aes(x = Actual, y = Predicted)) +
  geom_point() +
  geom_smooth() +
  geom_abline(intercept = 0, slope = 1, linetype = 2)
```

We repeat the same process for the session 2

```{r}
# Make tree on the whole session 1 data set 
tree.session2 <- rpart(formula = estimate.2, data = session.2, 
                       cp = 0.001, minsplit = 20)

# Plot the relative error as a function of cp parameter
plotcp(tree.session2, upper = "splits")
```

We will now prune the tree with the smallest relative SSE xerror to balance predictive power with simplicity.

The maximum CP under the dashed line is at 2 splits. However, this very low number must be due to the few data for this session. We will use the same number of splits as the session 1.

```{r}
# Find the new cp parameter value
cp.tree.session2 = tree.session2$cptable[tree.session2$cptable[,2] == 12, "CP"]

# Plot a tree with the new cp
rpart.plot(prune(tree.session2, cp = cp.tree.session2),
           type = 5,     # Displays variable name in interior nodes
           extra = 101,  # Displays number and percentage of observation
           space = 0,    # Deletes the space and make the text bigger
           main = "Session 2"
           )
```

Now we can see the variable importance for this tree

```{r}
prune(tree.session2, cp = cp.tree.session2)$variable.importance %>%
  data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Importance = ".") %>%
  ggplot(aes(x = fct_reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab(" ")

prune(tree.session2, cp = cp.tree.session2)$variable.importance %>%
  data.frame() %>%
  rownames_to_column(var = "Variable") %>%
  rename(Importance = ".")
```

Now let's evaluate the accuracy of the model

```{r}
pred.tree.session2 <- predict(prune(tree.session2, cp = cp.tree.session2), 
                              data = session.2)


data.frame(Predicted = pred.tree.session2, Actual = session.2$MV) %>%
  ggplot(aes(x = Actual, y = Predicted)) +
  geom_point() +
  geom_smooth() +
  geom_abline(intercept = 0, slope = 1, linetype = 2)
```

The model overestimates the low velocities and underestimates the high velocities

\newpage

# Random forest

A random forest combines multiple decision trees to make predictions. Each tree is made on bootstrapped training samples of the input data set ; and each tree is trained on a random sample of $m$ predictors chosen as split of the full set of $p$ predictors.

Therefore, each tree has only access to a part of the full training of the data set and a minority of the available. Then, each tree votes and the forest average those votes.

These characteristics allow a more reliable prediction of $Y$ than with single trees [@James&al.2010].

## Choosing the parameters

```{r, warning=FALSE}
# Make a data frame with the different tree models
comparison.forests <- data.frame(
  dummy = TestModel(
    model = "mean", 
    data = sessions, 
    number.itteration = kNumberRun
    ),
  estimate1.entire = TestModel(
    model = "forest", 
    data = sessions, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate1.session1 = TestModel(
    model = "forest", 
    data = session.1, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate1.session2 = TestModel(
    model = "forest", 
    data = session.2, 
    formula = estimate.1, 
    number.itteration = kNumberRun
    ),
  estimate2.entire = TestModel(
    model = "forest", 
    data = sessions, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    ),
  estimate2.session1 = TestModel(
    model = "forest", 
    data = session.1, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    ),
  estimate2.session2 = TestModel(
    model = "forest", 
    data = session.2, 
    formula = estimate.2, 
    number.itteration = kNumberRun
    )
) %>%
  pivot_longer(cols = everything(), 
               names_to = "model", values_to = "accuracy") %>% 
  separate(model, into = c("estimate", "dataset"), sep="\\.", remove = F)
  
# Compare the models
comparison.forests %>% 
  ggplot(aes(x = model, y = accuracy)) + 
  geom_boxplot() +
  theme_classic() +
  ggtitle("Comparison of forest models", 
          subtitle = "Different estimates and data sets") +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5))
```

All models allow a reliable prediction because they are more precise than the dummy prediction.

\newpage

### Estimate comparison

We are going to compare the models by estimate first.

```{r}
comparison.forests %>%
  filter(model != "dummy") %>%
  ggplot(aes(x = estimate, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  facet_grid(cols = vars(dataset), scales = "free_x") +
  ggtitle("Comparison of forest models", 
          subtitle = "Different estimates") +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5))
```

Using the `estimate.2` improve the accuracy of the model.

\newpage

### Data set comparison

We are going to compare the accuracy of the model with the entire data set vs. with separate data set for each session.

```{r}
comparison.forests %>%
  filter(estimate == "estimate1") %>%
  ggplot(aes(x = model, y = accuracy)) +
  geom_boxplot() +
  theme_classic() +
  ggtitle("Comparison of forest models", 
          subtitle = "Different data sets") +
  xlab("") + ylab("RMSE") + 
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5))
```

Separating the data set by sessions seem to improve the accuracy of the model.

\newpage

### Number of tree by forests

Now, let's determine the optimal number of tree by forest to predict our data sets.

```{r}
ntree.parameters <- seq(25, 500, by = 25)

# Create empty data frame, it will store the RMSE values
comparison.forests.ntree <- data.frame()

# First loop for session 1
for (nTree in ntree.parameters){
  
  # Create the model
  current.model <- TestModel(
    model = "forest",
    data = session.1,
    formula = estimate.2,
    number.itteration = kNumberRun,
    ntree.forest = nTree)
  
  # Create a tree name to identify it in the data frame
  forest.name <- paste("Sess 1 tree", nTree)
  
  comparison.forests.ntree <- rbind(
    comparison.forests.ntree,
    data.frame(name = forest.name, session = "Session 1",  # General info
               nTree = nTree,                              # What we test
               t(unlist(current.model))                    # The RMSE values
               )
    )

} 

# Second loop for session 2
for (nTree in ntree.parameters){
  
  # Create the model
  current.model <- TestModel(
    model = "forest",
    data = session.2,
    formula = estimate.2,
    number.itteration = kNumberRun,
    ntree.forest = nTree)
  
  # Create a tree name to identify it in the data frame
  forest.name <- paste("Sess 2 tree", nTree)
  
  comparison.forests.ntree <- rbind(
    comparison.forests.ntree,
    data.frame(name = forest.name, session = "Session 2",  # General info
               nTree = nTree,                              # What we test
               t(unlist(current.model))                    # The RMSE values
               )
    )

}


# Data frame in long format
comparison.forests.ntree <-  comparison.forests.ntree %>%
  pivot_longer(cols = 4:ncol(comparison.forests.ntree), 
               names_to = "tryNumber", values_to = "rmse")

# Make plots to visualize
plot.forest.session1 <- comparison.forests.ntree %>%
  filter(session == "Session 1") %>%
  ggplot(aes(x = nTree, y = rmse, group = nTree)) +
  geom_boxplot() +
  # facet_grid(cols = vars(div), scales = "free_x") +
  ggtitle("Comparison of forest for session 1 prediction",
          subtitle = "Number of tree by forest") +
  xlab("Number of tree by forest") + ylab("RMSE") +
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )

plot.forest.session2 <- comparison.forests.ntree %>%
  filter(session == "Session 2") %>%
  ggplot(aes(x = nTree, y = rmse, group = nTree)) +
  geom_boxplot() +
  # facet_grid(cols = vars(div), scales = "free_x") +
  ggtitle("Comparison of forest for session 2 prediction",
          subtitle = "Number of tree by forest") +
  xlab("Number of tree by forest") + ylab("RMSE") +
  theme(
    axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
    plot.title = element_text(hjust = 0.5), 
    plot.subtitle = element_text(hjust = 0.5)
    )

plot_grid(plot.forest.session1, plot.forest.session2)
```

Choosing 300 trees seem to improve the accuracy of the model.

\newpage

### Conclusion

The chosen parameters are the following :

-   We split the data set by session

-   We include the subject ID, the set and the repetition numbers in the tree prediction

-   The number of trees by forest is set at 300

\newpage

## Interpretations

Now, let's see what variables are the most useful to predict the MV with our data sets.

```{r}
# Fix the number of trees at 300 as mentionned before
kNumberTreeForest <- 300

# Create a random forest by session
forest.session1 <- randomForest(estimate.2, data = session.1, 
                                ntree = kNumberTreeForest)
rmse.forest.sesion.1 <- 
  sqrt(mean((predict(forest.session1, session.1) - session.1$MV)^2))

forest.session2 <- randomForest(estimate.2, data = session.2, 
                                ntree = kNumberTreeForest)
rmse.forest.sesion.2 <- 
  sqrt(mean((predict(forest.session2, session.2) - session.2$MV)^2))

# Create a data frame with variable importance for each session
variable.importance.forest.session1 <- data.frame(
  variable = rownames(forest.session1$importance),
  value = forest.session1$importance
  )
colnames(variable.importance.forest.session1)[2] <- "value"
variable.importance.forest.session1 <- variable.importance.forest.session1[
  order(variable.importance.forest.session1$value, decreasing = T), ]

variable.importance.forest.session2 <- data.frame(
  variable = rownames(forest.session2$importance), 
  value = forest.session2$importance
  )
colnames(variable.importance.forest.session2)[2] <- "value"
variable.importance.forest.session2 <- variable.importance.forest.session2[
  order(variable.importance.forest.session2$value, decreasing = T), ]

# Plot the variable importance
plot.variables.forest.session.1 <- variable.importance.forest.session1 %>%
  ggplot(aes(x = reorder(variable, -value, decreasing = TRUE), y = value)) +
  geom_bar(stat = "identity") +
  coord_flip () +
  theme_classic() +
  xlab("") + ylab("") +
  ggtitle("Variable importance for the session 1") +
  theme(axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5))

plot.variables.forest.session.2 <- variable.importance.forest.session2 %>%
  ggplot(aes(x = reorder(variable, -value, decreasing = TRUE), y = value)) +
  geom_bar(stat = "identity") +
  coord_flip () +
  theme_classic() +
  ggtitle("Variable importance for the session 2") +
  xlab("") + ylab("") +
  theme(axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5))
  
plot_grid(plot.variables.forest.session.1, plot.variables.forest.session.2)

variable.importance.forest.session2
```

Once again, we observe different variable importance depending on the session we analyze.

As mentioned for the tree, we only keep the 5 more important variables to visualize.

```{r}
# Plot the variable importance
plot.variables.forest.session.1 <- variable.importance.forest.session1 %>%
  head(5) %>%
  ggplot(aes(x = reorder(variable, -value, decreasing = TRUE), y = value)) +
  geom_bar(stat = "identity") +
  coord_flip () +
  theme_classic() +
  xlab("") + ylab("") +
  labs(title = "Fort session 1", 
          subtitle = paste(
            "RMSE : ", 
            round(rmse.forest.sesion.1, digits = 3))
          ) +
  theme(axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

plot.variables.forest.session.2 <- variable.importance.forest.session2 %>%
  head(5) %>%
  ggplot(aes(x = reorder(variable, -value, decreasing = TRUE), y = value)) +
  geom_bar(stat = "identity") +
  coord_flip () +
  theme_classic() +
  labs(title = "Fort session 2",
       subtitle = paste(
            "RMSE : ", 
            round(rmse.forest.sesion.2, digits = 3))
          )+
  xlab("") + ylab("") +
  theme(axis.text.x = element_text(angle = 42, hjust = 1, size = 10),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
  
plot_grid(nrow = 2, ncol = 2, 
          plot.variables.forest.session.1, 
          plot.variables.forest.session.2)
```

The variable importance order is not the same compared to tree models.

\newpage

# References
